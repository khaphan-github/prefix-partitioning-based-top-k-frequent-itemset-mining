# -*- coding: utf-8 -*-
"""PFT_Demo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bSwvVJorpjYHdHNrwbvX7-JIs9KFyN1b
"""

# Step 0: khai báo transaction table theo hình
transactions = {
    1:  ["x2", "x6", "x10"],
    2:  ["x1", "x2", "x9", "x10"],
    3:  ["x3", "x4"],
    4:  ["x2", "x10"],
    5:  ["x2", "x4", "x5", "x8", "x9", "x10"],
    6:  ["x2", "x9", "x10"],
    7:  ["x2", "x5", "x8", "x9", "x10"],
    8:  ["x2", "x3", "x4", "x9", "x10"],
    9:  ["x2", "x9"],
    10: ["x2", "x4", "x10"],
    11: ["x7", "x10"],
    12: ["x2", "x8", "x9"],
}

items = [f"x{i}" for i in range(1, 11)]  # x1..x10, đúng thứ tự bài báo [file:2]

print("Transactions:")
for tid, it in transactions.items():
    print(tid, ":", it)

# Step 1: xây P_i như Sect.4.1 [file:2]
def build_prefix_partitions(transactions, items):
    partitions = {x: [] for x in items}
    for tid, its in transactions.items():
        sorted_its = sorted(its, key=lambda v: int(v[1:]))  # x1<x2<... [file:2]
        m = len(sorted_its)
        for j in range(m):
            prefix = sorted_its[j]
            suffix = sorted_its[j:]       # các item bắt đầu từ prefix [file:2]
            partitions[prefix].append(suffix)
    return partitions

P = build_prefix_partitions(transactions, items)

print("Prefix-based partitions P_i:")
for xi in items:
    print(f"P_{xi}:")
    for row in P[xi]:
        print("  ", row)

# Step 2: xây CoN_i dưới dạng list các bản ghi (itemset, count)
from collections import defaultdict

def build_CoN_i_formatted(P, items):
    """
    P: dict xi -> list các transaction suffix trong partition P_i
    return:
        CoN_i: dict xi -> list[ {"itemset": tuple, "count": int} ]
        và đồng thời có thêm bản dạng thô (Counter) nếu cần dùng tiếp.
    """
    CoN_i_raw = {xi: defaultdict(int) for xi in items}

    for xi in items:
        for trans in P[xi]:
            if not trans:
                continue
            # trans = [xi, y2, ..., yb]
            # {xi}
            CoN_i_raw[xi][(xi,)] += 1
            # {xi, yj}
            for y in trans[1:]:
                a, b = sorted((xi, y), key=lambda v: int(v[1:]))
                CoN_i_raw[xi][(a, b)] += 1

    # Chuyển sang dạng list itemset–count, sắp xếp giống bảng minh họa [file:2]
    CoN_i = {}
    for xi in items:
        rows = []
        for itemset, c in CoN_i_raw[xi].items():
            rows.append({"itemset": itemset, "count": c})
        # sort: trước hết theo độ dài itemset, sau đó theo tên item
        rows.sort(key=lambda r: (len(r["itemset"]), [int(x[1:]) for x in r["itemset"]]))
        CoN_i[xi] = rows

    return CoN_i, CoN_i_raw

CoN_i, CoN_i_raw = build_CoN_i_formatted(P, items)

# In ra CoN1, CoN2 đúng style bảng (có thể in cho mọi i nếu muốn)
for xi in ["x1", "x2"]:
    print(f"\nCoN_{xi}")
    print("itemset\tcount")
    for row in CoN_i[xi]:
        print("{%s}\t%d" % (", ".join(row["itemset"]), row["count"]))

# Step 3: gộp tất cả CoN_i -> CoN (1- và 2-itemset) [file:2]
from collections import defaultdict

def merge_CoN_i_to_CoN(CoN_i, items):
    merged = defaultdict(int)   # itemset(tuple) -> count

    for xi in items:
        for row in CoN_i[xi]:           # row = {"itemset": tuple, "count": int}
            iset = row["itemset"]
            c = row["count"]
            merged[iset] += c

    CoN = [(iset, c) for iset, c in merged.items()]
    CoN.sort(key=lambda t: t[1], reverse=True)   # giảm dần theo count [file:2]
    return CoN

CoN = merge_CoN_i_to_CoN(CoN_i, items)

print("\nCoN (sau khi gộp và sắp xếp):")
print("itemset\tcount")
for iset, c in CoN:
    print("{%s}\t%d" % (", ".join(iset), c))

# Step 4: khởi tạo MH (min-heap), rmsup và AR_i từ CoN [file:2]
import heapq

k = 8   # top-k như ví dụ bài báo [file:2]

MH = []
for iset, s in CoN[:k]:
    heapq.heappush(MH, (s, iset))   # heap theo support

if len(CoN) >= k:
    rmsup = MH[0][0]   # kth highest support [file:2]
else:
    rmsup = 0

print("\nMH ban đầu:")
for s, iset in sorted(MH):
    print(iset, "=>", s)
print("rmsup =", rmsup)

# AR_i: mảng promising items cho từng partition [file:2]
AR_i = {xi: [] for xi in items}

for s, iset in MH:
    if len(iset) == 1:
        xi = iset[0]
        AR_i[xi].append(xi)
    elif len(iset) == 2:
        xi, xj = iset
        AR_i[xi].append(xj)

# lọc AR_i bằng rmsup và CoN_i đúng định nghĩa promising item [file:2]
for xi in items:
    new_arr = []
    for y in AR_i[xi]:
        # tìm count trong CoN_i[xi]
        sup = 0
        for row in CoN_i[xi]:
            if row["itemset"] == (xi,) and y == xi:
                sup = row["count"]; break
            if len(row["itemset"]) == 2 and y in row["itemset"] and xi in row["itemset"]:
                sup = row["count"]; break
        if sup >= rmsup:
            new_arr.append(y)
    AR_i[xi] = sorted(set(new_arr), key=lambda v: int(v[1:]))

print("\nAR_i sau khởi tạo (promising items mỗi partition):")
for xi in items:
    if AR_i[xi]:
        print(f"AR_{xi}:", AR_i[xi])

# Step 5.1: vertical representation (tid-lists) cho mỗi P_i [file:2]
from collections import defaultdict

def build_vertical_for_partition(Pi):
    tid_lists = defaultdict(list)   # item -> list local_tid
    for local_tid, trans in enumerate(Pi, start=1):
        for it in trans:
            tid_lists[it].append(local_tid)
    for it in tid_lists:
        tid_lists[it] = sorted(tid_lists[it])
    return tid_lists

def intersect_tids(a, b):
    i = j = 0
    res = []
    while i < len(a) and j < len(b):
        if a[i] == b[j]:
            res.append(a[i]); i += 1; j += 1
        elif a[i] < b[j]:
            i += 1
        else:
            j += 1
    return res

def support_of(itemset, tid_map):
    its = sorted(itemset, key=lambda v: int(v[1:]))
    res = tid_map[its[0]]
    for it in its[1:]:
        res = intersect_tids(res, tid_map[it])
        if not res:
            break
    return len(res), res

# Step 5.2: ProcessSglPartition (Algorithm 2 rút gọn) [file:2]
def process_partition(xi, Pi, AR_i_xi, MH, rmsup, k):
    print(f"\n=== ProcessSglPartition P_{xi} ===")
    print("AR_i:", AR_i_xi)

    if len(AR_i_xi) <= 2:
        print("Skip P_", xi, "vì |AR_i| <= 2.")
        return MH, rmsup

    tid_lists = build_vertical_for_partition(Pi)
    HT = {}
    QE = []   # max-heap: (-support, itemset)

    # khởi tạo QE bằng các 2-itemset {xi, y} [file:2]
    for y in AR_i_xi:
        if y == xi:
            continue
        iset = tuple(sorted((xi, y), key=lambda v: int(v[1:])))
        s, tids = support_of(iset, tid_lists)
        if s >= rmsup:
            HT[iset] = (s, tids)
            heapq.heappush(QE, (-s, iset))

    print("2-itemsets ban đầu trong QE:")
    for negs, iset in QE:
        print(iset, "support =", -negs)

    def try_update_MH(iset, s):
        nonlocal MH, rmsup
        if len(MH) < k:
            heapq.heappush(MH, (s, iset))
            rmsup = MH[0][0]
        else:
            if s > MH[0][0]:
                heapq.heapreplace(MH, (s, iset))
                rmsup = MH[0][0]

    while QE:
        negs, Xrt = heapq.heappop(QE)
        s_Xrt = -negs
        if s_Xrt <= rmsup:                 # điều kiện dừng partition [file:2]
            print("Dừng P_", xi, "vì QE.max <= rmsup")
            break

        Xrt_sorted = sorted(Xrt, key=lambda v: int(v[1:]))
        prefix = tuple(Xrt_sorted[:-1])
        y1 = Xrt_sorted[-1]

        if len(Xrt_sorted) >= 3:
            try_update_MH(tuple(Xrt_sorted), s_Xrt)
            print("Cập nhật MH với", Xrt_sorted, "support", s_Xrt, "=> rmsup =", rmsup)

        idx_y1 = AR_i_xi.index(y1)
        for y2 in AR_i_xi[idx_y1 + 1:]:
            # Timeliness pruning trên X∪{y2} [file:2]
            Xy2 = tuple(sorted(prefix + (y2,), key=lambda v: int(v[1:])))
            if Xy2 in HT:
                s_Xy2, _ = HT[Xy2]
            else:
                s_Xy2, tids_Xy2 = support_of(Xy2, tid_lists)
                HT[Xy2] = (s_Xy2, tids_Xy2)
            if s_Xy2 < rmsup:
                continue

            new_iset = tuple(sorted(Xrt_sorted + [y2], key=lambda v: int(v[1:])))
            s_new, tids_new = support_of(new_iset, tid_lists)
            if s_new >= rmsup and new_iset not in HT:
                HT[new_iset] = (s_new, tids_new)
                heapq.heappush(QE, (-s_new, new_iset))
                print("Push", new_iset, "support", s_new)

    return MH, rmsup

# Step 6: chạy PTF qua các P_i [file:2]
def run_ptf(transactions, items, P, CoN_i, AR_i, MH, rmsup, k):
    for xi in items:
        if not AR_i[xi]:
            continue

        # upper bound support trong P_i là support({xi}) trong CoN_i[xi] [file:2]
        max_sup_xi = 0
        for row in CoN_i[xi]:
            if row["itemset"] == (xi,):
                max_sup_xi = row["count"]; break

        if max_sup_xi <= rmsup or len(AR_i[xi]) <= 2:
            print(f"\nSkip P_{xi}: max_sup_xi={max_sup_xi}, |AR_i|={len(AR_i[xi])}, rmsup={rmsup}")
            continue

        MH, rmsup = process_partition(xi, P[xi], AR_i[xi], MH, rmsup, k)

        print("\nMH sau khi xử lý P_", xi)
        for s, iset in sorted(MH):
            print(iset, "=>", s)
        print("rmsup =", rmsup)

    return MH, rmsup

MH_final, rmsup_final = run_ptf(transactions, items, P, CoN_i, AR_i, MH, rmsup, k)

print("\n===== Top-k frequent itemsets (PTF, demo) =====")
for s, iset in sorted(MH_final, key=lambda t: (-t[0], t[1])):
    print(iset, "support =", s)
print("Final rmsup =", rmsup_final)